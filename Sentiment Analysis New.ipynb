{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train_2kmZucJ.csv')\n",
    "test=pd.read_csv('test_oJQbWVk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7920 entries, 0 to 7919\n",
      "Data columns (total 3 columns):\n",
      "id       7920 non-null int64\n",
      "label    7920 non-null int64\n",
      "tweet    7920 non-null object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 185.7+ KB\n",
      "None\n",
      "   id  label                                              tweet\n",
      "0   1      0  #fingerprint #Pregnancy Test https://goo.gl/h1...\n",
      "1   2      0  Finally a transparant silicon case ^^ Thanks t...\n",
      "2   3      0  We love this! Would you go? #talk #makememorie...\n",
      "3   4      0  I'm wired I know I'm George I was made that wa...\n",
      "4   5      1  What amazing service! Apple won't even talk to...\n"
     ]
    }
   ],
   "source": [
    "print(train.info())\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1953 entries, 0 to 1952\n",
      "Data columns (total 2 columns):\n",
      "id       1953 non-null int64\n",
      "tweet    1953 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 30.6+ KB\n",
      "None\n",
      "     id                                              tweet\n",
      "0  7921  I hate the new #iphone upgrade. Won't let me d...\n",
      "1  7922  currently shitting my fucking pants. #apple #i...\n",
      "2  7923  I'd like to puts some CD-ROMS on my iPad, is t...\n",
      "3  7924  My ipod is officially dead. I lost all my pict...\n",
      "4  7925  Been fighting iTunes all night! I only want th...\n"
     ]
    }
   ],
   "source": [
    "print(test.info())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_train=train.iloc[:,2]\n",
    "tweet_test=test.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=train.iloc[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing urls,numbers,symbols\n",
    "# Removing the urls\n",
    "tweet_train=tweet_train.str.replace(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*',' ')\n",
    "tweet_test=tweet_test.str.replace(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*',' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    #fingerprint #Pregnancy Test   #android #apps ...\n",
      "1    Finally a transparant silicon case ^^ Thanks t...\n",
      "2    We love this! Would you go? #talk #makememorie...\n",
      "3    I'm wired I know I'm George I was made that wa...\n",
      "4    What amazing service! Apple won't even talk to...\n",
      "Name: tweet, dtype: object\n",
      "0    I hate the new #iphone upgrade. Won't let me d...\n",
      "1    currently shitting my fucking pants. #apple #i...\n",
      "2    I'd like to puts some CD-ROMS on my iPad, is t...\n",
      "3    My ipod is officially dead. I lost all my pict...\n",
      "4    Been fighting iTunes all night! I only want th...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tweet_train[:5])\n",
    "print(tweet_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the Symbols\n",
    "tweet_train=tweet_train.str.replace(r'[^\\w]',' ')\n",
    "tweet_test=tweet_test.str.replace(r'[^\\w]',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     fingerprint  Pregnancy Test    android  apps ...\n",
      "1    Finally a transparant silicon case    Thanks t...\n",
      "2    We love this  Would you go   talk  makememorie...\n",
      "3    I m wired I know I m George I was made that wa...\n",
      "4    What amazing service  Apple won t even talk to...\n",
      "Name: tweet, dtype: object\n",
      "0    I hate the new  iphone upgrade  Won t let me d...\n",
      "1    currently shitting my fucking pants   apple  i...\n",
      "2    I d like to puts some CD ROMS on my iPad  is t...\n",
      "3    My ipod is officially dead  I lost all my pict...\n",
      "4    Been fighting iTunes all night  I only want th...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tweet_train[:5])\n",
    "print(tweet_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the Numbers\n",
    "tweet_train=tweet_train.str.replace(r'[0-9]+',' ')\n",
    "tweet_test=tweet_test.str.replace(r'[0-9]+',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     fingerprint  Pregnancy Test    android  apps ...\n",
      "1    Finally a transparant silicon case    Thanks t...\n",
      "2    We love this  Would you go   talk  makememorie...\n",
      "3    I m wired I know I m George I was made that wa...\n",
      "4    What amazing service  Apple won t even talk to...\n",
      "Name: tweet, dtype: object\n",
      "0    I hate the new  iphone upgrade  Won t let me d...\n",
      "1    currently shitting my fucking pants   apple  i...\n",
      "2    I d like to puts some CD ROMS on my iPad  is t...\n",
      "3    My ipod is officially dead  I lost all my pict...\n",
      "4    Been fighting iTunes all night  I only want th...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tweet_train[:5])\n",
    "print(tweet_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning them into lower case\n",
    "tweet_train=tweet_train.str.lower()\n",
    "tweet_test=tweet_test.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     fingerprint  pregnancy test    android  apps ...\n",
      "1    finally a transparant silicon case    thanks t...\n",
      "2    we love this  would you go   talk  makememorie...\n",
      "3    i m wired i know i m george i was made that wa...\n",
      "4    what amazing service  apple won t even talk to...\n",
      "Name: tweet, dtype: object\n",
      "0    i hate the new  iphone upgrade  won t let me d...\n",
      "1    currently shitting my fucking pants   apple  i...\n",
      "2    i d like to puts some cd roms on my ipad  is t...\n",
      "3    my ipod is officially dead  i lost all my pict...\n",
      "4    been fighting itunes all night  i only want th...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tweet_train[:5])\n",
    "print(tweet_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Whitespace\n",
    "# Replace whitespace between terms with a single space\n",
    "tweet_train = tweet_train.str.replace(r'\\s+', ' ')\n",
    "tweet_test = tweet_test.str.replace(r'\\s+', ' ')\n",
    "\n",
    "# Remove leading and trailing whitespace\n",
    "tweet_train = tweet_train.str.replace(r'^\\s+|\\s+?$', '')\n",
    "tweet_test = tweet_test.str.replace(r'^\\s+|\\s+?$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    fingerprint pregnancy test android apps beauti...\n",
      "1    finally a transparant silicon case thanks to m...\n",
      "2    we love this would you go talk makememories un...\n",
      "3    i m wired i know i m george i was made that wa...\n",
      "4    what amazing service apple won t even talk to ...\n",
      "Name: tweet, dtype: object\n",
      "0    i hate the new iphone upgrade won t let me dow...\n",
      "1    currently shitting my fucking pants apple imac...\n",
      "2    i d like to puts some cd roms on my ipad is th...\n",
      "3    my ipod is officially dead i lost all my pictu...\n",
      "4    been fighting itunes all night i only want the...\n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tweet_train[:5])\n",
    "print(tweet_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "tweet_train = tweet_train.apply(lambda x: ' '.join(\n",
    "    term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "tweet_test = tweet_test.apply(lambda x: ' '.join(\n",
    "    term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming the Words using Porter Stemmer\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=nltk.PorterStemmer()\n",
    "\n",
    "tweet_train=tweet_train.apply(lambda x:' '.join(ps.stem(term) for term in x.split()))\n",
    "tweet_test=tweet_test.apply(lambda x:' '.join(ps.stem(term) for term in x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7920,)\n",
      "(1953,)\n"
     ]
    }
   ],
   "source": [
    "print(tweet_train.shape)\n",
    "print(tweet_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Creating a Bag of Words Model\n",
    "all_words=[]\n",
    "\n",
    "for text in tweet_train:\n",
    "    words=word_tokenize(text)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "        \n",
    "for text in tweet_test:\n",
    "    words=word_tokenize(text)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "\n",
    "all_words=nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17859\n"
     ]
    }
   ],
   "source": [
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features=list(all_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fingerprint\n",
      "pregnanc\n",
      "test\n",
      "android\n",
      "app\n",
      "beauti\n",
      "cute\n",
      "health\n",
      "iger\n",
      "iphoneonli\n",
      "iphonesia\n",
      "iphon\n"
     ]
    }
   ],
   "source": [
    "def find_features(text):\n",
    "    words=word_tokenize(text)\n",
    "    features={}\n",
    "    for word in word_features:\n",
    "        features[word]=(word in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "features=find_features(tweet_train[0])\n",
    "\n",
    "for key,values in features.items():\n",
    "    if values==True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresset_train=[(find_features(tweet)) for (tweet) in tweet_train]\n",
    "featuresset_test=[(find_features(tweet)) for (tweet) in tweet_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset=list(zip(featuresset_train,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "training,validation=train_test_split(featureset,test_size=0.1,random_state=365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the Model and Fitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-cffda794cc43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# train the model on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# and test on the testing dataset!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\classify\\scikitlearn.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, labeled_featuresets)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\dict_vectorizer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[0mFeature\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0malways\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \"\"\"\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfitting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\dict_vectorizer.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, X, fitting)\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                     \u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                     \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mfitting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SklearnClassifier(SVC(kernel = 'linear'))\n",
    "\n",
    "# train the model on the training data\n",
    "model.train(training)\n",
    "\n",
    "# and test on the testing dataset!\n",
    "accuracy = nltk.classify.accuracy(model, validation)*100\n",
    "print(\"SVC Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "names=['KNeighbors Classifier','DecisionTree Classifier','RandomForest Classifier','Logistic Regression','SGD Classifier','Multinomial NB','SVC']\n",
    "classifier=[KNeighborsClassifier(),DecisionTreeClassifier(),RandomForestClassifier(),LogisticRegression(),SGDClassifier(max_iter=100),MultinomialNB(),SVC(kernel='linear')]\n",
    "\n",
    "models = zip(names, classifier)\n",
    "\n",
    "for name,classifier in models:\n",
    "    nltk_model=SklearnClassifier(classifier)\n",
    "    nltk_model.train(training)\n",
    "    accuracy=nltk.classify.accuracy(nltk_model,validation)*100\n",
    "    print('{} Accuracy : {}'.format(name,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble methods - Voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = [ \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifier = [\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')]\n",
    "\n",
    "models = list(zip(names, classifier))\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = models, voting = 'hard', n_jobs = -1))\n",
    "nltk_ensemble.train(training)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, validation)*100\n",
    "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=nltk_ensemble.classify_many(featuresset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ID=test.iloc[:,0]\n",
    "print(test_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_1=pd.DataFrame(test_ID,columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_1['label']=prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_1.to_csv('Submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
